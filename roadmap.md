Roadmap.md1.0 Project Vision & Mission1.1 VisionTo empower AI-driven platforms with secure, standardized, and autonomous control over the entire lifecycle of Kubernetes clusters, transforming infrastructure management from a manual, command-line-driven process into an intelligent, conversational, and automated one.1.2 MissionTo build and maintain a production-grade, open-source Model Context Protocol (MCP) server for the Kubernetes Cluster API (CAPI). This server will provide a robust set of tools that abstract the complexity of CAPI into a simple, secure, and extensible interface for AI agents, enabling them to perform complex cluster management tasks reliably and safely.12.0 Core PrinciplesThese non-negotiable values will guide all design, development, and operational decisions for the project.Security First: Security is not an afterthought but the foundational layer of the architecture. Every feature, component, and line of code will be designed and implemented with a zero-trust mindset, incorporating best practices for authentication, authorization, sandboxing, and data protection.3Extensible by Design: The architecture must be modular to facilitate the addition of new infrastructure providers (beyond the initial AWS scope), new tools, and enhanced capabilities with minimal refactoring. This is achieved through clearly defined interfaces and separation of concerns.Declarative at the Core: The server will honor the declarative, reconciliation-based nature of Cluster API.5 It will act as an intelligent interface to CAPI's control loops, not as an imperative script runner. The tools will translate agent intent into declarative state changes on the management cluster.Community-Driven & Open: The project will be developed entirely in the open, adhering to the ethos of the Cloud Native Computing Foundation (CNCF) and projects like Cluster API itself. We will actively foster a community, welcome contributions, and maintain transparent governance.5Usability for AI: Tool design will prioritize clarity, simplicity, and robustness to ensure that Large Language Models (LLMs) can easily and reliably understand and use the provided capabilities. Tool schemas will be explicit, and their descriptions will be detailed and unambiguous to prevent misinterpretation by the agent.63.0 V1.0 Scope: The AWS FoundationThe initial release will establish a solid, production-ready foundation, focusing on the most critical cluster management operations for a single, widely-used infrastructure provider.3.1 In-Scope FeaturesCore Toolset: Full implementation of the seven specified tools: list_clusters, get_cluster, create_cluster, delete_cluster, scale_cluster, get_cluster_kubeconfig, and get_cluster_nodes.Infrastructure Provider: Exclusive support for Amazon Web Services (AWS) via the official Cluster API Provider for AWS (CAPA).CAPI Compatibility: The server will be tested and validated against Cluster API version v1.10.3.7MCP Resource Exposure: The server will expose available cluster creation templates (ClusterClass objects) as a discoverable MCP Resource, simplifying and securing the cluster creation process.9Security Framework: Implementation of robust authentication and authorization for the MCP server endpoint and least-privilege RBAC for the server's interaction with the Kubernetes API.Testing Rigor: A comprehensive test suite including unit, integration, and end-to-end (E2E) tests that validate functionality against a live AWS environment.Baseline Observability: Structured logging for all operations and a baseline set of Prometheus metrics for monitoring server health and performance.3.2 Explicitly Out-of-Scope for V1.0Support for any infrastructure provider other than AWS (e.g., Azure, Google Cloud, vSphere).A graphical user interface (GUI) for the server.Advanced observability features such as distributed tracing or complex alerting rules.Direct management of in-cluster applications or add-ons. This functionality is intended to be handled by CAPI's ClusterResourceSet capability.84.0 Criteria for Success (V1.0)The V1.0 release will be considered successful upon meeting the following objective, measurable criteria.Functional Correctness: 100% of the end-to-end test suite passes consistently, demonstrating that all seven tools function as specified against a real AWS environment managed by a CAPI management cluster.Security Posture: The final release artifact contains zero high or critical vulnerabilities as identified by static application security testing (SAST) and software composition analysis (SCA) tools.4 The project must pass a formal internal security review.Performance: Key read-only operations (e.g., list_clusters, get_cluster) complete in under 500ms on average under normal load. Asynchronous, long-running operations (e.g., create_cluster) return an acknowledgment to the client in under 1 second.Reliability: The deployed server maintains 99.9% uptime over a 30-day pre-release test period under a simulated, continuous load of agentic requests.Documentation: All tools, resources, deployment procedures, and architectural decisions are clearly documented and published in the project repository. The Architecture.md and CLAUDE.md documents are finalized and approved.Adoption: The V1.0 server is successfully integrated and used by at least one internal platform team or two external early adopters to manage development or staging clusters, providing positive feedback on its stability and utility.5.0 Phased Development PlanThe development of V1.0 will proceed in three distinct phases, allowing for iterative progress, early feedback, and focused effort.Phase 1 (Milestone 1): Foundation & Read-Only Operations (Weeks 1-4)Goals: Establish the core project structure, CI/CD pipeline, and security framework. Implement all non-mutating tools to validate the foundational components.Key Tasks:Initialize the Go project repository with standard layout, build system, linters (golangci-lint), and a basic CI pipeline.Implement the core MCP server scaffolding using the official modelcontextprotocol/go-sdk.11Develop the CAPI Client Wrapper (internal/kube) for abstracting interactions with the Kubernetes API.Implement API key-based authentication for the MCP server endpoint.Define the Provider interface and the initial AWSProvider stub to establish the pattern for extensibility.Implement the read-only tools: list_clusters, get_cluster, get_cluster_kubeconfig, and get_cluster_nodes.Develop comprehensive unit and integration tests for all implemented components.Phase 2 (Milestone 2): Mutating Operations & E2E Testing (Weeks 5-9)Goals: Implement the remaining tools that modify cluster state and build a comprehensive end-to-end (E2E) test suite to validate real-world behavior.Key Tasks:Implement the create_cluster tool, leveraging ClusterClass templates for safety and reliability.Implement the delete_cluster tool, ensuring it correctly initiates CAPI's deletion process.Implement the scale_cluster tool, focusing on modifying the replicas field of MachineDeployment objects.Design and build the E2E testing framework using Kubernetes-in-Docker (kind) for the management cluster and provisioning real AWS resources for workload clusters.Write and stabilize E2E tests for all seven tools.Phase 3 (Milestone 3): Hardening, Documentation & Release (Weeks 10-12)Goals: Finalize the project for its V1.0 release by focusing on performance, security, documentation, and deployment artifacts.Key Tasks:Conduct performance and load testing to identify and resolve any bottlenecks.Perform security hardening, including code audits, dependency scanning, and validating RBAC rules.Write comprehensive user and developer documentation, including setup guides and tool references.Create and publish official container images and a Helm chart for easy deployment.Tag, release, and announce V1.0.
# Architecture.md

## 1.0 System Overview & Architectural Patterns

The Cluster API (CAPI) MCP Server is a standalone service designed to act as a secure bridge between AI agents (MCP clients) and a CAPI management cluster. It translates high-level, tool-based requests from an agent into declarative, object-based operations on the Kubernetes API of the management cluster.

### 1.1 Architectural Pattern

The server's design is fundamentally based on a **Proxy/Gateway Pattern**. It exposes a standardized MCP interface to the outside world while hiding the complexity of the underlying CAPI and Kubernetes APIs.[12] It does not maintain its own state regarding clusters; instead, it serves as a stateless proxy whose source of truth is always the CAPI management cluster itself.

Conceptually, this system operates within a **Hierarchical Agent Pattern**.[13] The external AI agent acts as the "planner," responsible for decomposing a user's request into a sequence of steps. The CAPI MCP Server acts as the "executor" or "tool-user," providing a constrained set of powerful, reliable tools that the planner can invoke to accomplish its goals.

The data flow is as follows:
1.  An MCP Client (e.g., an AI agent in an IDE) sends a JSON-RPC request to the CAPI MCP Server to execute a tool (e.g., `scale_cluster`).
2.  The server's security middleware authenticates and authorizes the request.
3.  The server's Tool Provider routes the request to the appropriate business logic in the CAPI Service Layer.
4.  The CAPI Service Layer translates the tool's parameters into a CAPI object manipulation (e.g., fetching a `MachineDeployment` object and updating its `spec.replicas` field).
5.  This manipulation is performed via the CAPI Client Wrapper, which communicates with the Kubernetes API server of the management cluster.
6.  For long-running operations, the server monitors the status of the relevant CAPI resources and returns a final success or failure state to the agent.

### 1.2 Core Technology Stack

*   **Language**: Go (Golang), chosen for its high performance, robust concurrency model, static typing, and its position as the de facto language of the cloud-native ecosystem, including Kubernetes and Cluster API itself.
*   **MCP Framework**: The official `modelcontextprotocol/go-sdk` will be used to handle all protocol-level concerns, ensuring full compliance with the MCP specification.[11]
*   **Kubernetes Interaction**: The server will use the standard Kubernetes `k8s.io/client-go` and `sigs.k8s.io/controller-runtime/pkg/client` libraries. These provide type-safe, robust, and well-tested mechanisms for interacting with the Kubernetes API.

## 2.0 Core Component Design

The server is composed of several distinct, loosely coupled components, each with a specific responsibility. This modular design promotes maintainability, testability, and future extensibility.

*   **MCP Server Engine**: The main application entry point. Built using the `go-sdk`, this component is responsible for handling the underlying transport (e.g., HTTP with JSON-RPC), parsing incoming messages, and routing requests to the Tool Provider.
*   **Tool Provider**: The central hub for all exposed capabilities. It programmatically registers each of the seven tools, defining their names, detailed descriptions, and input schemas. It maps incoming tool calls from the Server Engine to the corresponding implementation in the CAPI Service Layer.
*   **CAPI Service Layer**: This is the core business logic layer. It contains functions like `CreateCluster`, `ListClusters`, and `ScaleMachineDeployment`. This layer is completely decoupled from the MCP protocol itself. Its responsibility is to orchestrate the steps needed to fulfill a request, such as validating inputs, interacting with the CAPI Client Wrapper, and handling the asynchronous nature of CAPI operations.
*   **Provider Interface**: A critical component for extensibility, this is a Go interface (`type Provider interface {...}`) that defines a contract for any infrastructure-specific logic. For example, it might include methods like `GetInfrastructureTemplateSpec()` or `ValidateProviderConfig()`. This allows new providers (e.g., for Azure or GCP) to be added by simply creating a new struct that satisfies this interface.
*   **AWS Provider Implementation**: The V1 implementation of the `Provider` interface. It contains logic specific to the Cluster API Provider for AWS (CAPA), such as details about `AWSCluster` and `AWSMachineTemplate` objects.
*   **CAPI Client Wrapper**: A dedicated internal package (`internal/kube`) that abstracts all direct interactions with the Kubernetes API. It provides high-level, easy-to-use functions like `GetCAPIClusterByName(ctx, name)` or `UpdateMachineDeployment(ctx, md)`. This isolates the rest of the application from the complexities of `client-go` and makes testing easier by allowing the client to be mocked.
*   **Configuration Manager**: A component responsible for loading, validating, and providing access to server configuration. It will follow the 12-factor app methodology, prioritizing environment variables for configuration, which can be populated from Kubernetes ConfigMaps or Secrets.[4, 14]
*   **Security Module**: A set of middleware functions that plug into the MCP Server Engine. These are responsible for enforcing the security policies defined in the Security Architecture section, primarily authentication and authorization on every incoming request.

### 2.1 Bridging the Asynchronous Gap

A primary architectural challenge is the impedance mismatch between the synchronous, function-call nature of MCP tools and the asynchronous, declarative nature of Cluster API. An AI agent calling `create_cluster` expects a definitive result to continue its workflow, but CAPI operations can take several minutes to complete.[5, 13] Simply creating the CAPI resources and returning `202 Accepted` is insufficient, as it leaves the agent blind to the operation's progress or potential failure.

To solve this, long-running mutating operations (`create_cluster`, `delete_cluster`, `scale_cluster`) will not return immediately. Instead, their implementation within the CAPI Service Layer will:
1.  Initiate the declarative operation by creating or modifying the necessary CAPI resources on the management cluster.
2.  Immediately begin watching the `status` fields of the relevant resources (e.g., the `Cluster` object's `status.phase` and `status.conditions`, or the `MachineDeployment`'s `status.updatedReplicas`).[10]
3.  The tool will block and wait for the resource to reach a terminal state (e.g., `Provisioned`, `Failed`) or until a configurable timeout is reached.
4.  The final status, including any error messages from the CAPI controllers, will be returned to the agent, providing a clear, actionable result. This makes the tools far more reliable for use in multi-step agentic workflows.

## 3.0 Source Code Organization

A standardized directory structure will be enforced to ensure a clean separation of concerns and facilitate navigation for developers.
/capi-mcp-server
├── /api
│   └── /v1           # Go structs defining MCP tool and resource JSON schemas
├── /cmd
│   └── /server       # main.go - application entry point and initialization
├── /internal
│   ├── /server       # Core MCP server engine, routing, and transport middleware
│   ├── /service      # The CAPI service layer (business logic)
│   ├── /kube         # CAPI Client Wrapper for all k8s API interactions
│   └── /config       # Configuration loading and validation
├── /pkg
│   ├── /provider     # The provider interface definition
│   │   └── /aws      # The AWS provider implementation
│   └── /tools        # The implementation of each of the 7 MCP tools
├── /deploy
│   ├── /charts       # Helm chart for deploying the server and its resources
│   └── /manifests    # Raw Kubernetes manifests for development
├── /test
│   ├── /integration  # Integration tests
│   └── /e2e          # End-to-end tests
├── go.mod
└── Dockerfile
4.0 Tool & Resource Implementation Blueprints4.1 MCP ResourcesMCP Resources expose file-like data that an agent can read for context.2Resource Name: capi_cluster_templatesDescription: Lists available ClusterClass resources from the management cluster. These templates provide a safe, validated, and standardized way to create new clusters, preventing agents from needing to construct complex and error-prone manifests.Implementation Logic: The resource handler will list all ClusterClass objects in the management cluster and format them into a human-readable and agent-parsable list.4.2 MCP Tools and CAPI MappingMCP Tools are functions that an agent can execute.2 The design of these tools must be precise. A critical architectural decision is to reject the creation of clusters from raw YAML manifests. Asking an LLM to generate a full CAPI manifest is fragile and a significant security risk due to potential prompt injection or hallucinated configurations.4Instead, the create_cluster tool will be template-driven. It will accept a small number of simple parameters (e.g., template_name, cluster_name, kubernetes_version) and use a trusted, administrator-defined ClusterClass from the management cluster as its base. This approach dramatically improves security, reliability, and ease of use for the AI agent.6The following table provides the definitive mapping from each MCP tool to the underlying CAPI resources it manipulates. This serves as a "rosetta stone" for developers, operators, and security auditors.MCP ToolDescriptionPrimary CAPI Resource(s)K8s Verbs RequiredKey Fields Manipulatedlist_clustersLists all managed workload clusters and their current status.cluster.x-k8s.io/v1beta1.Clusterlist, getmetadata.name, status.phaseget_clusterGets detailed information for a specific cluster.cluster.x-k8s.io/v1beta1.Cluster, infrastructure.cluster.x-k8s.io/v1beta1.AWSClustergetspec, statuscreate_clusterCreates a new workload cluster from a pre-defined ClusterClass template.cluster.x-k8s.io/v1beta1.Clustercreatemetadata.name, spec.topology.class, spec.topology.variablesdelete_clusterDeletes a specified workload cluster and all its associated resources.cluster.x-k8s.io/v1beta1.Clusterdeletemetadata.namescale_clusterScales the number of worker nodes in a specific node pool (MachineDeployment).cluster.x-k8s.io/v1beta1.MachineDeploymentget, patchspec.replicasget_cluster_kubeconfigRetrieves the kubeconfig file needed to access a workload cluster.v1.Secretgetdata.value (read-only)get_cluster_nodesLists the nodes within a specific workload cluster.v1.Node (via workload cluster client)listmetadata.name, status.conditions5.0 Security ArchitectureA multi-layered security strategy is employed to protect the CAPI management cluster and the infrastructure it controls.5.1 Threat ModelUnauthorized Access: An attacker gains access to the MCP server to perform malicious actions (e.g., delete clusters).Data Exfiltration: An attacker uses the server to exfiltrate sensitive data, such as kubeconfig files or cloud credentials.Cluster Takeover: An attacker exploits a vulnerability to gain administrative control over workload clusters.Denial of Service: An attacker overwhelms the server with requests, preventing legitimate use and potentially impacting the management cluster's API server.5.2 AuthenticationClient -> Server: All requests to the MCP server's endpoint must be authenticated. The V1 implementation will use a static Bearer Token (API Key) provided in the Authorization header. This key will be configurable via environment variables. Future versions will evaluate OAuth 2.1 for more dynamic, scoped access control.3Server -> Kubernetes API: The server will be deployed as a pod within the CAPI management cluster. It will use a dedicated Kubernetes ServiceAccount. The token for this ServiceAccount is automatically mounted into the pod's filesystem by Kubernetes and used by client-go to securely authenticate with the API server.5.3 Authorization (RBAC)The principle of least privilege is strictly enforced. A Kubernetes Role and RoleBinding will be created specifically for the server's ServiceAccount. The permissions granted in this Role will be the absolute minimum required for the server to function, derived directly from the "K8s Verbs Required" column in the Tool to CAPI Resource Mapping Table. For example, the role will grant get on secrets but not list or create, and delete on clusters but not on clusterclasses.5.4 Secrets ManagementThe get_cluster_kubeconfig tool is highly sensitive. The server will retrieve the corresponding Kubernetes Secret, extract the kubeconfig data, and return it directly to the client without ever logging its contents.The server's configuration will not contain secrets. All sensitive values (like the API key for server authentication) will be injected as environment variables from Kubernetes Secrets during deployment via the Helm chart.45.5 Sandboxing and Network PolicyThe server will be deployed into its own dedicated Kubernetes namespace to provide isolation from other workloads.A strict Kubernetes NetworkPolicy will be applied to the server's pod. This policy will:Restrict ingress traffic to only allow connections from trusted sources (e.g., an API gateway or internal VPC CIDR).Restrict egress traffic to only allow outbound connections to the Kubernetes API server's endpoint within the cluster. This prevents the server from making arbitrary outbound network calls, a critical mitigation against many classes of data exfiltration attacks.36.0 Deployment & Operations (Day 2)6.1 ContainerizationA multi-stage Dockerfile will be used to build the application. The final stage will use a minimal, hardened base image (e.g., gcr.io/distroless/static-debian12) to reduce the attack surface of the production container image.6.2 DeploymentA Helm chart will be the official and required method for deploying the server. The chart will be responsible for creating all necessary Kubernetes resources in the correct order:NamespaceServiceAccountRole and RoleBindingSecret for the server's API keyDeployment for the server podService to expose the server internallyNetworkPolicy6.3 ObservabilityLogging: All log output will be written to stdout in a structured JSON format using Go's standard slog library. This allows for easy collection and parsing by standard log aggregation platforms (e.g., Fluentd, Loki). Logs will include contextual information like tool name and request ID.Metrics: The server will expose a /metrics endpoint compatible with Prometheus. It will export key metrics, including:mcp_requests_total{tool="<tool_name>", status="<success|error>"}: A counter for tool invocations.mcp_request_latency_seconds{tool="<tool_name>"}: A histogram of tool execution latency.go_*: Standard Go runtime metrics.
# CLAUDE.md

## 1.0 Prime Directive & Persona

You are an elite, senior Go software engineer. Your primary area of expertise is in building secure, scalable, and maintainable distributed systems, with deep, practical knowledge of Kubernetes, its API machinery (`client-go`, `controller-runtime`), and cloud-native best practices. You write clean, idiomatic, and heavily tested code. You are expected to think like an architect and execute like a master craftsman. Your work must adhere to the highest standards of quality and security.

## 2.0 Core Mandates

These are the non-negotiable rules for all development on this project.

*   **Adhere to the Architecture**: You must follow the `Architecture.md` document precisely. The component design, source code organization, and security patterns are not suggestions; they are requirements. Any proposed deviation must be justified in a formal design proposal and receive explicit approval from the project lead.
*   **Security is Your Responsibility**: You are personally responsible for writing secure code. Every line of code, every dependency added, and every configuration change must be considered from a security perspective. You must complete the **Mandatory Security Checklist** for every pull request without exception.[4]
*   **Test Everything**: No code is considered complete without comprehensive tests. You will write unit tests for all business logic and contribute to the integration and E2E test suites for every feature you develop. The project's code coverage target is >85% for all new code.
*   **Document as You Go**: All public functions and types must have clear, complete GoDoc comments that explain their purpose, parameters, and return values. Complex or non-obvious logic within function bodies must be explained with concise comments.
*   **Communicate with Precision**: Your Git commit messages will follow the Conventional Commits specification. Your pull request descriptions will be detailed, explaining the "what" and the "why" of your changes, linking to the relevant issue, and including the completed security checklist.

## 3.0 Development Environment & Toolchain

To ensure a consistent and effective development process, all engineers will use the following set of tools and versions.

*   **Go**: `1.23.x`
*   **Docker & Docker Compose**: Latest stable version
*   **Kubernetes CLI**: `kubectl` (latest stable)
*   **Helm CLI**: `helm` (latest stable)
*   **Local Kubernetes**: `kind` (Kubernetes-in-Docker) for running a local management cluster
*   **IDE**: Visual Studio Code with the official Go extension (`golang.go`) is recommended.

## 4.0 Engineering Standards & Best Practices

### 4.1 Go Language & Style

*   Strictly follow the principles outlined in `Effective Go` and the official Go `Code Review Comments` guide.
*   The project's `golangci-lint` configuration is the single source of truth for linting. Your code must pass all linters before it can be merged.
*   Errors are values and must be handled explicitly. Do not use the blank identifier (`_`) to discard errors unless it is absolutely necessary and justified with a comment. Use the `errors` package for wrapping errors to preserve context.
*   Use structured logging via the standard library's `slog` package for all application output. Do not use `fmt.Printf` or `log.Printf`.

### 4.2 Kubernetes & Cluster API Interaction

*   All interactions with the Kubernetes API **must** be performed through the functions provided by the `internal/kube` CAPI Client Wrapper. Do not instantiate or use a raw `client-go` client directly within the service or tool layers. This ensures a consistent, testable, and centralized point of control for all API communication.
*   Every function that communicates with the Kubernetes API must accept a `context.Context` as its first argument. This context must be used in the API call to propagate cancellation and deadlines.
*   When creating or updating Kubernetes resources, use Server-Side Apply where appropriate to prevent conflicts with other controllers.

### 4.3 MCP Implementation

*   The Go structs that define the input and output schemas for all MCP tools and resources must be located in the `/api/v1/` directory. These structs must include JSON tags for serialization and validation tags for input checking.
*   Tool descriptions must be explicit, detailed, and unambiguous. They must clearly state what the tool does, what each parameter means, any constraints on the parameters, and what the tool returns on success or failure. This is critical for the LLM agent's ability to use the tool correctly.[6]
*   Validate all inputs received from the MCP client rigorously at the beginning of every tool's execution. Do not trust that the client has sent valid data.

## 5.0 Task Execution Framework (TEF)

You must follow this step-by-step process for implementing any new feature or fixing any bug.

1.  **Analyze**: Thoroughly read the associated Jira ticket and all relevant sections of `Roadmap.md` and `Architecture.md`. Ensure you have a complete understanding of the requirements.
2.  **Design**: Before writing significant code, outline your implementation plan. This can be a brief comment in the Jira ticket or a draft pull request description. For complex features, a separate design document may be required.
3.  **Code (Test-Driven)**: Begin by writing a failing unit test that captures the core requirement. Then, write the minimum amount of application code necessary to make the test pass. Refactor and repeat.
4.  **Test**: Once the unit tests are passing, add or update integration and/or E2E tests as required by the feature's scope. Manually verify the functionality in a local `kind` environment.
5.  **Secure**: Meticulously complete the **Mandatory Security Checklist** below.
6.  **Document**: Write or update all necessary GoDoc comments. If the change affects users or operators, update the relevant markdown documentation (e.g., `docs/tools.md`).
7.  **Review**: Submit a pull request. Ensure the description is complete, linking to the issue and including the filled-out security checklist. Respond to all review comments promptly and professionally.

## 6.0 Mandatory Security Checklist (for every PR)

This checklist must be copied into the description of every pull request and all items must be checked off.
- [ ] **Input Validation**: All inputs from the MCP client (tool parameters) are rigorously validated (e.g., for type, range, format) before being used.
- [ ] **Secret Handling**: No secrets (e.g., kubeconfig content, API keys) are ever logged or included in error messages returned to the client.
- [ ] **Resource Access**: The code only accesses the specific Kubernetes resources defined for its function in `Architecture.md`. No overly broad queries (e.g., listing all secrets in all namespaces) are performed.
- [ ] **Context Propagation**: `context.Context` with appropriate timeouts is passed down through all function calls, especially those making external network requests to the Kubernetes API.
- [ ] **Dependency Audit**: Any new third-party dependencies have been approved and scanned for known vulnerabilities using the project's SCA tooling.
- [ ] **Error Handling**: Error messages returned to the client are generic and do not leak internal system state, stack traces, or implementation details.
7.0 Tool Implementation PlaybooksThe following playbook for implementing the scale_cluster tool serves as a concrete template for all other tool implementations.Example Playbook: Implementing the scale_cluster ToolDefine Schema (in /api/v1/tools.go):Define the Go struct for the tool's input:Go// ScaleClusterInput defines the parameters for the scale_cluster tool.
type ScaleClusterInput struct {
    ClusterName      string `json:"cluster_name" validate:"required"`
    NodePoolName     string `json:"node_pool_name" validate:"required"`
    Replicas         int    `json:"replicas" validate:"gte=0"`
}
Register Tool (in /pkg/tools/registry.go):In the tool registration function, add a new entry for scale_cluster.Provide the tool name, a detailed description explaining that it scales a worker node MachineDeployment, and reference the ScaleClusterInput struct for its schema.Implement Service Logic (in /internal/service/cluster_service.go):Create a new public method on the service: func (s *ClusterService) ScaleCluster(ctx context.Context, input api.ScaleClusterInput) error.Service Logic Steps:Inside ScaleCluster, first call the CAPI Client Wrapper to fetch the target MachineDeployment: md, err := s.kubeClient.GetMachineDeployment(ctx, input.ClusterName, input.NodePoolName). Handle any IsNotFound or other errors.Update the spec.replicas field on the fetched object: md.Spec.Replicas = &input.Replicas.Call the update function in the client wrapper: err := s.kubeClient.UpdateMachineDeployment(ctx, md). Handle any errors.Implement Watcher: After the update call succeeds, begin a watch/poll loop on the MachineDeployment object. Continue polling until md.Status.UpdatedReplicas == *md.Spec.Replicas. The loop must respect the ctx for cancellation and have a hard timeout (e.g., 10 minutes). If the desired state is not reached within the timeout, return a detailed error.Write Unit Tests (in /internal/service/cluster_service_test.go):Write a unit test for the ScaleCluster service method. Use a mock Kubernetes client (e.g., fake.NewClientBuilder()) to simulate the API interactions.Test the success case, not-found errors, and update failures.Write E2E Test (in /test/e2e/scaling_test.go):Add a new E2E test case. The test will:Create a standard test cluster using an existing E2E utility function.Wait for the cluster to be fully provisioned.Invoke the scale_cluster tool via an MCP client pointed at the server.Verify that the tool returns a success message.Directly query the workload cluster's nodes and verify that the number of worker nodes matches the new replica count.
